{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MixedModelsBLB.jl is a Julia package for fitting and performing inference for linear mixed models using the Bag of little bootstrap (BLB) method. The advantages of our method include (1) it can run on extremely large data sets that do not fit into memory; (2) testing random effect standard deviation on the boundary poses no problem because the inference is based on bootstrap rather than normal approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (v1.3) pkg> add https://github.com/xinkai-zhou/MixedModelsBLB.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will install this package and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.3.1\n",
      "Commit 2d5741174c (2019-12-30 21:36 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin18.6.0)\n",
      "  CPU: Intel(R) Core(TM) i5-5257U CPU @ 2.70GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.1 (ORCJIT, broadwell)\n"
     ]
    }
   ],
   "source": [
    "# machine information for this tutorial\n",
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using MixedModelsBLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BLB on smaller data\n",
    "\n",
    "If the dataset is small enough to fit into the memory, users can simply call ```blb_full_data``` on the data object (of type ```DataFrame```). The model is specified through the familiar formula interface. Other important arguments include\n",
    "- ```subset_size```, the desired number of cluters in each subset;\n",
    "- ```n_subsets```, the total number of BLB subsets;\n",
    "- ```n_boots```, the number of bootstrap iterations.\n",
    "\n",
    "We will use the ```sleepstudy``` data to illustrate the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DataFrames, MixedModels, RData, StatsBase, Random\n",
    "# datf = joinpath(dirname(pathof(MixedModels)),\"..\",\"test\",\"dat.rda\")\n",
    "# const dat = Dict(Symbol(k)=>v for (k,v) in load(datf));\n",
    "# dsleepstudy = dat[:sleepstudy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# β̂, Σ̂, τ̂ = blb_full_data(\n",
    "#     dsleepstudy, \n",
    "#     @formula(Y ~ U + (1 | G));\n",
    "#     subset_size = 10,\n",
    "#     n_subsets = 10, \n",
    "#     n_boots = 1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we uses an optimization algorithm that requires gradients. Alternatively, one can switch to a gradient free algorithm by specifying, for example, ```solver = NLopt.NLoptSolver(algorithm=:LN_BOBYQA, maxeval=10000)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For documentation of the ```blb_full_data``` function, type ```?blb_full_data``` in Julia REPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the BLB estimates, the final point estimates can be obtained by taking two averages: the first  across all bootstrap iterations in one subset, and the second across all subsets. The confidence intervals can be obtained by averaging the percentile confidence intervals across all subsets. The function ```summary``` is used to extrace point estimates and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(β̂)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BLB on large data\n",
    "\n",
    "If the data file is too big to fit into the memory, users can pass the file path as an argument, and the function ```blb_full_data``` will use ```JuliaDB``` to load only small subsets into the memory to run the BLB procedure. To demonstrate the usage, we will use ```sleepstudy.csv``` as a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# β̂, Σ̂, τ̂ = blb_full_data(\n",
    "#     \"sleepstudy.csv\", \n",
    "#     @formula(Y ~ U + (1 | G)); \n",
    "#     id_name = \"id\", \n",
    "#     subset_size = 10,\n",
    "#     n_subsets = 10, \n",
    "#     n_boots = 1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs \n",
    "blb_one_subset\n",
    "\n",
    "@docs\n",
    "blb_full_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
